How to Expose Deployment in AWS (using the AWS Load Balancer Controller)
----------------------------------------------------------------------

Problem:  I want to have one load balancer that can manage all of my deployed webapps
          https://apps.traderres.com/app1 --->  web app #1
          https://apps.traderres.com/app2 --->  web app #2
          https://apps.traderres.com/app3 --->  web app #3

Goal:  Share the same Application Load Balancer (ALB) for multiple services


References
----------
https://medium.com/devops-dudes/running-the-latest-aws-load-balancer-controller-in-your-aws-eks-cluster-9d59cdc1db98
https://aws.amazon.com/blogs/containers/introducing-aws-load-balancer-controller/
https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html
https://aws.amazon.com/blogs/containers/how-to-expose-multiple-applications-on-amazon-eks-using-a-single-application-load-balancer/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.5/deploy/installation/
https://repost.aws/knowledge-center/eks-load-balancer-controller-subnets
https://stackoverflow.com/questions/66039501/eks-alb-is-not-to-able-to-auto-discover-subnets
https://aws.amazon.com/blogs/containers/exposing-kubernetes-applications-part-2-aws-load-balancer-controller/


Part 1:  Install the AWS Load Balancer Controller
-------------------------------------------------
 1. Specify variables
    unix> export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')      # holds us-east-2
    unix> export AWS_REGISTRY_ID=$(aws ecr describe-registry --query registryId --output text)                                    # holds 524647912468
    unix> export AWS_ECR_REPO=${AWS_REGISTRY_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com                                              # holds 524647912468.dkr.ecr.us-east-2.amazonaws.com
    unix> export AWS_CLUSTER_NAME=nccs


 2. Create an IAM OIDC provider. You can skip this step if you already have one for your cluster.
    unix> eksctl utils associate-iam-oidc-provider --region $AWS_REGION --cluster $AWS_CLUSTER_NAME --approve

    You should see this:
      2023-06-12 10:21:18 [ℹ]  will create IAM Open ID Connect provider for cluster "nccs" in "us-east-2"
      2023-06-12 10:21:18 [✔]  created IAM Open ID Connect provider for cluster "nccs" in "us-east-2"


 3. Create an IAM policy:  AWSLoadBalancerControllerIAMPolicy
    a. Download an IAM policy for the Load Balancer Controller (LBC) using one of the following commands:
       If using US Gov Cloud, then use this:
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy_us-gov.json

       If using China, then use this:
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy_cn.json

       If using any other region
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy.json

    b. Create the IAM policy:
       unix> aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam-policy.json

       You should see this:
		{
			"Policy": {
				"PolicyName": "AWSLoadBalancerControllerIAMPolicy",
				"PolicyId": "ANPAXUJ3T2QKCM4ROVU4E",
				"Arn": "arn:aws:iam::524647912468:policy/AWSLoadBalancerControllerIAMPolicy",
				"Path": "/",
				"DefaultVersionId": "v1",
				"AttachmentCount": 0,
				"PermissionsBoundaryUsageCount": 0,
				"IsAttachable": true,
				"CreateDate": "2023-06-12T14:24:04+00:00",
				"UpdateDate": "2023-06-12T14:24:04+00:00"
			}
		}



 4. Create an IAM role and Kubernetes ServiceAccount for the Load Balancer Controller. Use the ARN from the previous step.
    unix> eksctl delete iamserviceaccount --cluster=$AWS_CLUSTER_NAME --namespace=kube-system  --name=aws-load-balancer-controller
    unix> eksctl create iamserviceaccount --cluster=$AWS_CLUSTER_NAME --namespace=kube-system  --name=aws-load-balancer-controller \
          --attach-policy-arn=arn:aws:iam::${AWS_REGISTRY_ID}:policy/AWSLoadBalancerControllerIAMPolicy \
          --override-existing-serviceaccounts --region $AWS_REGION --approve

    *OR*

    unix> eksctl create iamserviceaccount --cluster=$AWS_CLUSTER_NAME --namespace=kube-system  --name=aws-load-balancer-controller \
              --attach-policy-arn=arn:aws:iam::524647912468:policy/AWSLoadBalancerControllerIAMPolicy \
              --override-existing-serviceaccounts --region $AWS_REGION --approve

    You should see this output  (takes 60 seconds)
		2023-06-12 10:27:24 [ℹ]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)
		2023-06-12 10:27:24 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
		2023-06-12 10:27:24 [ℹ]  1 task: {
			2 sequential sub-tasks: {
				create IAM role for serviceaccount "kube-system/aws-load-balancer-controller",
				create serviceaccount "kube-system/aws-load-balancer-controller",
			} }2023-06-12 10:27:24 [ℹ]  building iamserviceaccount stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:24 [ℹ]  deploying stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:24 [ℹ]  waiting for CloudFormation stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:54 [ℹ]  waiting for CloudFormation stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:54 [ℹ]  created serviceaccount "kube-system/aws-load-balancer-controller"


 5. Add the controller to the cluster
    a. Install the cert-manager
       unix> kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml

       You should see this:
			customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created
			namespace/cert-manager created
			serviceaccount/cert-manager-cainjector created
			serviceaccount/cert-manager created
			serviceaccount/cert-manager-webhook created
			clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
			clusterrole.rbac.authorization.k8s.io/cert-manager-view created
			clusterrole.rbac.authorization.k8s.io/cert-manager-edit created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created
			clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created
			role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
			role.rbac.authorization.k8s.io/cert-manager:leaderelection created
			role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
			rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
			rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created
			rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
			service/cert-manager created
			service/cert-manager-webhook created
			deployment.apps/cert-manager-cainjector created
			deployment.apps/cert-manager created
			deployment.apps/cert-manager-webhook created
			mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created
			validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created


    b. Download the spec for the load balancer controller
       unix> wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.5.1/v2_5_1_full.yaml

    c. Set your cluster name in the v2_5_1_full.yaml
       unix> vi v2_5_1_full.yaml

       Change this line:
          - --cluster-name=your-cluster-name

	   To this line:
	   - --cluster-name=nccs

    d. Because we used service account, we need to delete the ServiceAccount from the v2_5_1_full.yaml
       unix> vi v2_5_1_full.yaml

       Remove this section:
		   ---
		   apiVersion: v1
		   kind: ServiceAccount
		   metadata:
			 labels:
			   app.kubernetes.io/component: controller
			   app.kubernetes.io/name: aws-load-balancer-controller
			 name: aws-load-balancer-controller
			 namespace: kube-system

    e. Apply the script
       unix> kubectl apply -f v2_5_1_full.yaml

       You should see this:
		 customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws created
		 customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws created
		 role.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-role created
		 clusterrole.rbac.authorization.k8s.io/aws-load-balancer-controller-role created
		 rolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-rolebinding created
		 clusterrolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-rolebinding created
		 service/aws-load-balancer-webhook-service created
		 deployment.apps/aws-load-balancer-controller created
		 certificate.cert-manager.io/aws-load-balancer-serving-cert created
		 issuer.cert-manager.io/aws-load-balancer-selfsigned-issuer created
		 mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created
		 validatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created


 6. Download & install the default ingressclass and ingressclass params
    a. Download the ingressclass and ingressclass params
       unix> wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.5.1/v2_5_1_ingclass.yaml

    b. Apply the ingressclass nad params
       unix> kubectl apply -f v2_5_1_ingclass.yaml

       You should see this:
       	 ingressclassparams.elbv2.k8s.aws/alb created
       	 ingressclass.networking.k8s.io/alb created


 7. Verify that the aws-load-balancer-controller was created

    a. Verify that aws-load-balancer-webhook-service exists
       unix> kubectl get services -n kube-system

		NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
		aws-load-balancer-webhook-service   ClusterIP   172.20.171.29   <none>        443/TCP         3m18s
		kube-dns                            ClusterIP   172.20.0.10     <none>        53/UDP,53/TCP   19h


    b. Verify that the aws-load-balancer-controller exists
       unix> kubectl get deployments -n kube-system

		NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
		aws-load-balancer-controller   1/1     1            1           4m9s
		coredns                        2/2     2            2           19h


    c. Get the details on the aws-load-balancer-controller
       unix> kubectl describe deployment aws-load-balancer-controller -n kube-system

		  Name:                   aws-load-balancer-controller
		  Namespace:              kube-system
		  CreationTimestamp:      Mon, 12 Jun 2023 10:53:12 -0400
		  Labels:                 app.kubernetes.io/component=controller
								  app.kubernetes.io/name=aws-load-balancer-controller
		  Annotations:            deployment.kubernetes.io/revision: 1
		  Selector:               app.kubernetes.io/component=controller,app.kubernetes.io/name=aws-load-balancer-controller
		  Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
		  StrategyType:           RollingUpdate
		  MinReadySeconds:        0
		  RollingUpdateStrategy:  25% max unavailable, 25% max surge
		  Pod Template:
			Labels:           app.kubernetes.io/component=controller
							  app.kubernetes.io/name=aws-load-balancer-controller
			Service Account:  aws-load-balancer-controller
			Containers:
			 controller:
			  Image:      public.ecr.aws/eks/aws-load-balancer-controller:v2.5.1
			  Port:       9443/TCP
			  Host Port:  0/TCP
			  Args:
				--cluster-name=your-cluster-name
				--ingress-class=alb
			  Limits:
				cpu:     200m
				memory:  500Mi
			  Requests:
				cpu:        100m
				memory:     200Mi
			  Liveness:     http-get http://:61779/healthz delay=30s timeout=10s period=10s #success=1 #failure=2
			  Environment:  <none>
			  Mounts:
				/tmp/k8s-webhook-server/serving-certs from cert (ro)
			Volumes:
			 cert:
			  Type:               Secret (a volume populated by a Secret)
			  SecretName:         aws-load-balancer-webhook-tls
			  Optional:           false
			Priority Class Name:  system-cluster-critical
		  Conditions:
			Type           Status  Reason
			----           ------  ------
			Available      True    MinimumReplicasAvailable
			Progressing    True    NewReplicaSetAvailable
		  OldReplicaSets:  <none>
		  NewReplicaSet:   aws-load-balancer-controller-6d59f749b8 (1/1 replicas created)
		  Events:
			Type    Reason             Age   From                   Message
			----    ------             ----  ----                   -------
			Normal  ScalingReplicaSet  31m   deployment-controller  Scaled up replica set aws-load-balancer-controller-6d59f749b8 to 1



    W A I T     2    M I N U T E S



Part 2:  Deploy a sample application to verify that the AWS Load Balancer works
-------------------------------------------------------------------------------
 1. Get the game called 2048
    unix> wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.0/docs/examples/2048/2048_full.yaml

 2. Erase the existing one
    unix> kubectl delete -f 2048_full.yaml

 3. Apply the new one
    unix> kubectl apply -f 2048_full.yaml

 4. Wait a few minutes

 5. Verify that the ingress resource was created
    unix> kubectl get ingress/ingress-2048 -n game-2048

    NAME           CLASS   HOSTS   ADDRESS                                                                   PORTS   AGE
    ingress-2048   alb     *       k8s-game2048-ingress2-7b127ca593-1789793846.us-east-2.elb.amazonaws.com   80      97s
                                   (if you see a public address, then the load balancer is provisioning)

 6. View the AWS Load Balancer Controller logs (to look for errors)
    unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system

 7. Go to AWS.com -> EC2 -> Load Balancer
    Wait for the load balancer to finish provisioning

 8. Go to http://http://k8s-game2048-ingress2-7b127ca593-1789793846.us-east-2.elb.amazonaws.com/
    -- You should see the 2048 game

 9. Delete the app
    a. Delete the app
       unix> kubectl delete -f 2048_full.yaml

    b. Verify that there are no load balancers
       Go to AWS.com -> EC2 -> Load Balancer   (you should see none)



Troubleshooting
---------------
    Problem #1:    How do change the log level to debug for the aws-load-balancer-controller?
    Solution #1:   Edit the settings and add --log-level=debug
                   unix> kubectl edit deployment aws-load-balancer-controller -n kube-system

				   spec:
					  containers:
					  - args:
						- --cluster-name=nccs
						- --ingress-class=alb
						- --log-level=debug           # Set the log level here to debug.  Default is info
						image: public.ecr.aws/eks/aws-load-balancer-controller:v2.5.1


    Problem #2:    I get the error "couldn't auto-discover subnets: unable to resolve at least one subnet"
    Solution #2:   The cluster-name is not correct
                   unix> kubectl edit deployment aws-load-balancer-controller -n kube-system

					  Search for:      your-cluster-name
					  Replace with:    nccs

				   spec:
					  containers:
					  - args:
						- --cluster-name=nccs
						- --ingress-class=alb
						- --log-level=debug           # Set the log level here to debug.  Default is info
						image: public.ecr.aws/eks/aws-load-balancer-controller:v2.5.1


				  # Look at the log to verify if the error is still there
				  unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system



   Problem #3:    I get the error "couldn't auto-discover subnets: unable to resolve at least one subnet"
   Solution #3:   Make sure your ingress clsas has your 3 public subnet ids:
                  Add this to metadata.annotations:
                     alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c

               alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c


   Apply the changes
   unix> kubectl apply -f 2048_full.yaml

   Look at the log file:
   unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system

   Get the front-door URL
   unix> kubectl get ingress -n game-2048
   NAME           CLASS   HOSTS   ADDRESS                                                                  PORTS   AGE
   ingress-2048   alb     *       k8s-game2048-ingress2-7b127ca593-165518510.us-east-2.elb.amazonaws.com   80      6m33s

   Attempt to connect to it
   -- Go to http://k8s-game2048-ingress2-7b127ca593-165518510.us-east-2.elb.amazonaws.com/


 5. Delete the application
    unix> kubectl delete -f 2048_full.yaml





Part 3:  Configure the Ingress to use the AWS Load Balancer Controller
----------------------------------------------------------------------
 1. Take your app.traderres.com server cert and import into the AWS Certificate Manager
    a. Go to AWS -> Certificate Manager
    b. Press "Import Certificate"
    c. In "Input certificate details"

       Certificate Body Text box:
          ssh to the certbox/keyclaok server
          unix> sudo -s
          unix> cat /home/ec2-user/certbot/archive/app.traderres.com/fullchain_part1.pem   # holds server cert
          Copy the BEGIN CERT...END CERT and paste into the "Certificate Body"


      Certificate Private Key Text box:
          ssh to the certbox/keyclaok server
          unix> sudo -s
          unix> cat /home/ec2-user/certbot/archive/app.traderres.com/privkey1.pem
          Copy the BEGIN PRIVATE KEY...END PRIVATE KEY and paste into the "Certificate Private key"


       Certificate Chain:
           ssh to the certbox/keyclaok server
           unix> sudo -s
           unix> cat /home/ec2-user/certbot/archive/app.traderres.com/fullchain1.pem
           Copy all 3  BEGIN CERT...END CERT and paste into the "Certificate Chain"

       Press "Next"


    d. In the "Add Tags"
       press "Add tag"
         tag key:  Name
         tag value: app.traderres.com

       Press "Next"


    e. In the "Review and import"
       press "Import"


 2. Get the ARN for that server cert and include in your app-load-balancer.yaml
    a. Go to AWS -> Certificate Manager
    b. Click on Certificates
    c. Click on the certificate for app.traderres.com
    d. Press the Copy button next to the ARN -- e.g., arn:aws:acm:us-east-2:524647912468:certificate/18c98c70-f0a2-4dfb-b434-8317c6894367


 3. Verify that the nccs-deployment looks something like this:
######################################################################################
# Filename:  nccs-deployment-v2.2.3.yaml
#
# Purpose:   Tell kubernetes how to deploy the NCCS web app(s)
#
# Usage
#  1. Follow the steps to setup AWS credentials (so you can build & deploy)
#     https://github.com/traderres/webClass/blob/master/learnAWS/howToBuildAndPushToAmazonECR.txt
#
#  2. Build and push the containers to the Amazon ECR (image repository)
#     unix> mvn -Djib.to.auth.username=AWS -Djib.to.auth.password=$PASSWORD clean package -Pprod -PbuildImageAndPush
#
#  3. Edit the environment variables in this yaml file to match reality -- e.g., set the POSTGRES_HOSTNAME
#     a. Also edit the image name (in this yaml file) to make sure it matches what was pushed to the Amazon ECR
#
#  4. Tell kubernetes to deploy the sync service
#     unix> kubectl delete deployment nccs-deployment
#     unix> kubectl apply -f nccs-deployment-v2.2.3.yaml
#
#  5. Get the name of the running pod  (it changes every time)
#     unix> kubectl get pods
#
#     NAME                                            READY   STATUS         RESTARTS   AGE
#     nccs-deployment-69c9fd9db6-6wkcf                1/1     Running        0          19s
#
#  6. Look at the pod log to verify that the NCCS webapp is up
#     uniX> kubectl logs -f nccs-deployment-69c9fd9db6-6wkcf
#
# Notes:
#  A) The running pods do not have DNS so we use hostAliases to create an entry in /etc/hosts
#     that can resolve the keycloak.traderres.com to 52.15.227.187
#     Use nslookup keycloak.traderres.com to verify the IP
#
#  B) If you want 5 copies of the webapp, then change replicas to 5 (in this file)
######################################################################################
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nccs-deployment
  labels:
    app: nccs-deployment

spec:
  selector:
    matchLabels:
      app: nccs-deployment

  replicas: 1
  strategy:
    type:  Recreate
  template:
    metadata:

      labels:
         app: nccs-deployment

    spec:
      hostAliases:
      - ip: "52.15.227.187"
        hostnames:
          - "keycloak.traderres.com"

      volumes:
      - name: app-certs-pv
        persistentVolumeClaim:
          claimName: app-certs-pvc

      containers:
      - image: 524647912468.dkr.ecr.us-east-2.amazonaws.com/nccs:backend-2.3.0-SNAPSHOT
        imagePullPolicy: Always
        volumeMounts:
        - name: app-certs-pv
          mountPath: /shared/certs

        name: nccs-deployment
        ports:
            - containerPort: 80
              name: health-check
            - containerPort: 443
              name: web-app

        env:
           - name: POSTGRES_HOSTNAME
             value: "app-database.csbk2jnrc8ao.us-east-2.rds.amazonaws.com"
           - name: POSTGRES_DB_USERNAME
             value: "app_user"
           - name: POSTGRES_DB_PASSWORD
             value: "secret12"
           - name: POSTGRES_DB_NAME
             value: "app_db"
           - name: POSTGRES_SCHEMA_NAME
             value: "app_db"
           - name: POSTGRES_POOL_SIZE
             value: "5"
           - name: ES_URL
             value: "https://vpc-app-opensearch-ep5v5i6jlal7ka3qraya46fpsi.us-east-2.es.amazonaws.com:443"
           - name: ES_USERNAME
             value: "es_user"
           - name: ES_PASSWORD
             value: "Secret1@"
           - name: KEYCLOAK_CLIENT_ID
             value: "nccs-webapp"
           - name: KEYCLOAK_CLIENT_SECRET
             value: "wnAJ5rM1BQs9YKSWLrtYL0z0YgS6LxDG"
           - name: KEYCLOAK_ISSUER_URI
             value: "https://keycloak.traderres.com:8444/realms/MyRealm"
           - name: INCOMING_KEYSTORE_FILEPATH
             value: "/shared/certs/webapp.keystore.jks"
           - name: INCOMING_KEYSTORE_PASSWORD
             value: "changeit"
           - name: INCOMING_TRUSTSTORE_FILEPATH
             value: "/shared/certs/webapp.truststore.jks"
           - name: INCOMING_TRUSTSTORE_PASSWORD
             value: "changeit"
           - name: JAVA_TOOL_OPTIONS
             value: -Xms1024m -Xmx1024m -Demail.mode=off -Djavax.net.ssl.trustStore=/shared/certs/custom.cacerts -Djavax.net.ssl.trustStorePassword=changeit








 4. Create teh app-load-balancer.yaml file
    unix> vi  app-load-balancer.yaml

######################################################################################
# Filename:  app-load-balancer.yaml
#
# Purpose:   Setup the services and the ingress rules for load balancing
#
#
#  Application
#   Load      ---> /nccs ----> nccs-service ---> nccs-deployment --> one of many pods
#  Balancer
#
######################################################################################


#####################################################
# NCCS Service
#####################################################
apiVersion: v1
kind: Service
metadata:
  namespace: default
  name: nccs-service
  labels:
    app: nccs-service
  annotations:
     alb.ingress.kubernetes.io/healthcheck-path: /nccs/manage/health

spec:
  type: NodePort
  selector:
    app: nccs-deployment
  ports:
    - name: https
      port: 443
      targetPort: 443
    - name: http
      port: 80
      targetPort: 80
  sessionAffinity: ClientIP


---
#####################################################
# NCCS-Admin Service
#####################################################
apiVersion: v1
kind: Service
metadata:
  namespace: default
  name: nccs-admin-service
  labels:
    app: nccs-admin-service
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: /nccs-admin/manage/health

spec:
  type: NodePort
  selector:
    app: nccs-admin-deployment
  ports:
    - name: https
      port: 443
      targetPort: 443
    - name: http
      port: 80
      targetPort: 80
  sessionAffinity: ClientIP



---
#####################################################
# Ingress (Application Load Balancer) Rules
#####################################################
#
# INGRESS
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: default
  labels:
    app: apps
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: instance
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    alb.ingress.kubernetes.io/healthcheck-port: '80'
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '60'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443},{"HTTP":80}]'
    alb.ingress.kubernetes.io/certificate-arn: 'arn:aws:acm:us-east-2:524647912468:certificate/18c98c70-f0a2-4dfb-b434-8317c6894367'
    alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c    # replace with 3 public subnet ids
spec:
  ingressClassName: alb
  rules:
    - host: app.traderres.com
      http:
        paths:
          - path: /nccs-admin
            pathType: Prefix
            backend:
              service:
                name: nccs-admin-service
                port:
                  number: 80
          - path: /nccs
            pathType: Prefix
            backend:
              service:
                name: nccs-service
                port:
                  number: 443





 4. Apply the script (to create the load balancer and targets)
    unix> kubectl apply -f app-load-balancer.yaml

 5. Wait for the load balancer to show status of Active in AWS -> EC2 -> Load Balancer

 6. Go into goDaddy.com and register app.traderres.com with the public hostname of the Load Balancer
	a. Update godaddy's cname record for app.traderres.com so that it corresponds to the new external hostname
		a. Go to https://dcc.godaddy.com/control/portfolio/traderres.com/settings
        b. Click on the DNS tab
		c. Edit the old CNAME record for "app"    (this corresponds to app.traderres.com)
		   Type:  CNAME
		   Name:  app
		   Value: k8s-default-appingre-b55bbaaa0f-1406394226.us-east-2.elb.amazonaws.com

		d. Press "Save"


 7. Wait for goDaddy to show that app.traderres.com points to the new load-balancer public hostname
    Do a lookup to get the current hostname for app.traderres.com
    unix> nslookup app.traderres.com

    *OR keep looping forever*
    unix> while [ 1 ]; do clear; date; nslookup app.traderres.com; sleep 3; done


 8. Attempt to connect to https://app.traderres.com/nccs

