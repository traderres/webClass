How to Setup a Load Balancer on AWS
-----------------------------------

Problem:  I want to have one load balancer that can manage all of my deployed webapps
          https://apps.traderres.com/app1 --->  web app #1
          https://apps.traderres.com/app2 --->  web app #2
          https://apps.traderres.com/app3 --->  web app #3

Goal:  Share the same Application Load Balancer (ALB) for multiple services

References
----------
https://medium.com/devops-dudes/running-the-latest-aws-load-balancer-controller-in-your-aws-eks-cluster-9d59cdc1db98
https://aws.amazon.com/blogs/containers/introducing-aws-load-balancer-controller/
https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html
https://aws.amazon.com/blogs/containers/how-to-expose-multiple-applications-on-amazon-eks-using-a-single-application-load-balancer/
https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.5/deploy/installation/
https://repost.aws/knowledge-center/eks-load-balancer-controller-subnets
https://stackoverflow.com/questions/66039501/eks-alb-is-not-to-able-to-auto-discover-subnets


Part 1:  Install the AWS Load Balancer Controller
-------------------------------------------------
 1. Specify variables
    unix> export AWS_REGION=$(aws ec2 describe-availability-zones --output text --query 'AvailabilityZones[0].[RegionName]')      # holds us-east-2
    unix> export AWS_REGISTRY_ID=$(aws ecr describe-registry --query registryId --output text)                                    # holds 524647912468
    unix> export AWS_ECR_REPO=${AWS_REGISTRY_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com                                              # holds 524647912468.dkr.ecr.us-east-2.amazonaws.com
    unix> export AWS_CLUSTER_NAME=nccs


 2. Create an IAM OIDC provider. You can skip this step if you already have one for your cluster.
    unix> eksctl utils associate-iam-oidc-provider --region $AWS_REGION --cluster $AWS_CLUSTER_NAME --approve

    You should see this:
      2023-06-12 10:21:18 [ℹ]  will create IAM Open ID Connect provider for cluster "nccs" in "us-east-2"
      2023-06-12 10:21:18 [✔]  created IAM Open ID Connect provider for cluster "nccs" in "us-east-2"


 3. Create an IAM policy:  AWSLoadBalancerControllerIAMPolicy
    a. Download an IAM policy for the Load Balancer Controller (LBC) using one of the following commands:
       If using US Gov Cloud, then use this:
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy_us-gov.json

       If using China, then use this:
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy_cn.json

       If using any other region
       unix> curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.1/docs/install/iam_policy.json

    b. Create the IAM policy:
       unix> aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam-policy.json

       You should see this:
		{
			"Policy": {
				"PolicyName": "AWSLoadBalancerControllerIAMPolicy",
				"PolicyId": "ANPAXUJ3T2QKCM4ROVU4E",
				"Arn": "arn:aws:iam::524647912468:policy/AWSLoadBalancerControllerIAMPolicy",
				"Path": "/",
				"DefaultVersionId": "v1",
				"AttachmentCount": 0,
				"PermissionsBoundaryUsageCount": 0,
				"IsAttachable": true,
				"CreateDate": "2023-06-12T14:24:04+00:00",
				"UpdateDate": "2023-06-12T14:24:04+00:00"
			}
		}



 4. Create an IAM role and Kubernetes ServiceAccount for the Load Balancer Controller. Use the ARN from the previous step.
    unix> eksctl create iamserviceaccount --cluster=$AWS_CLUSTER_NAME --namespace=kube-system  --name=aws-load-balancer-controller \
          --attach-policy-arn=arn:aws:iam::<AWS_ACCOUNT_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
          --override-existing-serviceaccounts --region $AWS_REGION --approve

    *OR*

    unix> eksctl create iamserviceaccount --cluster=$AWS_CLUSTER_NAME --namespace=kube-system  --name=aws-load-balancer-controller \
              --attach-policy-arn=arn:aws:iam::524647912468:policy/AWSLoadBalancerControllerIAMPolicy \
              --override-existing-serviceaccounts --region $AWS_REGION --approve

    You should see this output  (takes 60 seconds)
		2023-06-12 10:27:24 [ℹ]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)
		2023-06-12 10:27:24 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
		2023-06-12 10:27:24 [ℹ]  1 task: {
			2 sequential sub-tasks: {
				create IAM role for serviceaccount "kube-system/aws-load-balancer-controller",
				create serviceaccount "kube-system/aws-load-balancer-controller",
			} }2023-06-12 10:27:24 [ℹ]  building iamserviceaccount stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:24 [ℹ]  deploying stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:24 [ℹ]  waiting for CloudFormation stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:54 [ℹ]  waiting for CloudFormation stack "eksctl-nccs-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
		2023-06-12 10:27:54 [ℹ]  created serviceaccount "kube-system/aws-load-balancer-controller"


 5. Add the controller to the cluster
    a. Install the cert-manager
       unix> kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml

       You should see this:
			customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created
			customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created
			namespace/cert-manager created
			serviceaccount/cert-manager-cainjector created
			serviceaccount/cert-manager created
			serviceaccount/cert-manager-webhook created
			clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
			clusterrole.rbac.authorization.k8s.io/cert-manager-view created
			clusterrole.rbac.authorization.k8s.io/cert-manager-edit created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created
			clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created
			clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created
			clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created
			role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
			role.rbac.authorization.k8s.io/cert-manager:leaderelection created
			role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
			rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
			rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created
			rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
			service/cert-manager created
			service/cert-manager-webhook created
			deployment.apps/cert-manager-cainjector created
			deployment.apps/cert-manager created
			deployment.apps/cert-manager-webhook created
			mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created
			validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created


    b. Download the spec for the load balancer controller
       unix> wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.5.1/v2_5_1_full.yaml

    c. Set your cluster name in the v2_5_1_full.yaml
       unix> vi v2_5_1_full.yaml

       Change this line:
          - --cluster-name=your-cluster-name

	   To this line:
	   - --cluster-name=nccs

    d. Because we used service account, we need to delete the ServiceAccount from the v2_5_1_full.yaml
       unix> vi v2_5_1_full.yaml

       Remove this section:
		   ---
		   apiVersion: v1
		   kind: ServiceAccount
		   metadata:
			 labels:
			   app.kubernetes.io/component: controller
			   app.kubernetes.io/name: aws-load-balancer-controller
			 name: aws-load-balancer-controller
			 namespace: kube-system

    e. Apply the script
       unix> kubectl apply -f v2_5_1_full.yaml

       You should see this:
		 customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws created
		 customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws created
		 role.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-role created
		 clusterrole.rbac.authorization.k8s.io/aws-load-balancer-controller-role created
		 rolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-rolebinding created
		 clusterrolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-rolebinding created
		 service/aws-load-balancer-webhook-service created
		 deployment.apps/aws-load-balancer-controller created
		 certificate.cert-manager.io/aws-load-balancer-serving-cert created
		 issuer.cert-manager.io/aws-load-balancer-selfsigned-issuer created
		 mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created
		 validatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created


 6. Download & install the default ingressclass and ingressclass params
    a. Download the ingressclass and ingressclass params
       unix> wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.5.1/v2_5_1_ingclass.yaml

    b. Apply the ingressclass nad params
       unix> kubectl apply -f v2_5_1_ingclass.yaml

       You should see this:
       	 ingressclassparams.elbv2.k8s.aws/alb created
       	 ingressclass.networking.k8s.io/alb created


 7. Verify that the aws-load-balancer-controller was created

    a. Verify that aws-load-balancer-webhook-service exists
       unix> kubectl get services -n kube-system

		NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
		aws-load-balancer-webhook-service   ClusterIP   172.20.171.29   <none>        443/TCP         3m18s
		kube-dns                            ClusterIP   172.20.0.10     <none>        53/UDP,53/TCP   19h


    b. Verify that the aws-load-balancer-controller exists
       unix> kubectl get deployments -n kube-system

		NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
		aws-load-balancer-controller   1/1     1            1           4m9s
		coredns                        2/2     2            2           19h


    c. Get the details on the aws-load-balancer-controller
       unix> kubectl describe deployment aws-load-balancer-controller -n kube-system

		  Name:                   aws-load-balancer-controller
		  Namespace:              kube-system
		  CreationTimestamp:      Mon, 12 Jun 2023 10:53:12 -0400
		  Labels:                 app.kubernetes.io/component=controller
								  app.kubernetes.io/name=aws-load-balancer-controller
		  Annotations:            deployment.kubernetes.io/revision: 1
		  Selector:               app.kubernetes.io/component=controller,app.kubernetes.io/name=aws-load-balancer-controller
		  Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
		  StrategyType:           RollingUpdate
		  MinReadySeconds:        0
		  RollingUpdateStrategy:  25% max unavailable, 25% max surge
		  Pod Template:
			Labels:           app.kubernetes.io/component=controller
							  app.kubernetes.io/name=aws-load-balancer-controller
			Service Account:  aws-load-balancer-controller
			Containers:
			 controller:
			  Image:      public.ecr.aws/eks/aws-load-balancer-controller:v2.5.1
			  Port:       9443/TCP
			  Host Port:  0/TCP
			  Args:
				--cluster-name=your-cluster-name
				--ingress-class=alb
			  Limits:
				cpu:     200m
				memory:  500Mi
			  Requests:
				cpu:        100m
				memory:     200Mi
			  Liveness:     http-get http://:61779/healthz delay=30s timeout=10s period=10s #success=1 #failure=2
			  Environment:  <none>
			  Mounts:
				/tmp/k8s-webhook-server/serving-certs from cert (ro)
			Volumes:
			 cert:
			  Type:               Secret (a volume populated by a Secret)
			  SecretName:         aws-load-balancer-webhook-tls
			  Optional:           false
			Priority Class Name:  system-cluster-critical
		  Conditions:
			Type           Status  Reason
			----           ------  ------
			Available      True    MinimumReplicasAvailable
			Progressing    True    NewReplicaSetAvailable
		  OldReplicaSets:  <none>
		  NewReplicaSet:   aws-load-balancer-controller-6d59f749b8 (1/1 replicas created)
		  Events:
			Type    Reason             Age   From                   Message
			----    ------             ----  ----                   -------
			Normal  ScalingReplicaSet  31m   deployment-controller  Scaled up replica set aws-load-balancer-controller-6d59f749b8 to 1



Part 2:  Deploy a sample application to verify that the AWS Load Balancer works
-------------------------------------------------------------------------------
 1. Get the game called 2048
    unix> wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.0/docs/examples/2048/2048_full.yaml

    unix> kubectl apply -f 2048_full.yaml

 2. Wait a few minutes

 3. Verify that the ingress resource was created
    unix> kubectl get ingress/ingress-2048 -n game-2048


 4. View the AWS Load Balancer Controller logs (to look for errors)
    unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system


Troubleshooting
---------------
    Problem #1:    I get the error "couldn't auto-discover subnets: unable to resolve at least one subnet"
    Solution #1:   The cluster-name is not correct
    unix> kubectl edit deployment -n kube-system aws-load-balancer-controller

    Search for:      your-cluster-name
    Replace with:    nccs

    # Look at the log to verify if the error is still there
    unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system


   Problem #2:  I get the error "couldn't auto-discover subnets: unable to resolve at least one subnet"
   Solution #2: Make sure your ingress clsas has your 3 public subnet ids:
                Add this to metadata.annotations:
                   alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c


   So, the ingress class should look something like this:
   unix> vi 2048_full.yaml

       Add this to metadata.annotations:
               alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c


   Apply the changes
   unix> kubectl apply -f 2048_full.yaml

   Look at the log file:
   unix> kubectl logs -f deployment.apps/aws-load-balancer-controller -n kube-system

   Get the front-door URL
   unix> kubectl get ingress -n game-2048
   NAME           CLASS   HOSTS   ADDRESS                                                                  PORTS   AGE
   ingress-2048   alb     *       k8s-game2048-ingress2-7b127ca593-165518510.us-east-2.elb.amazonaws.com   80      6m33s

   Attempt to connect to it
   -- Go to http://k8s-game2048-ingress2-7b127ca593-165518510.us-east-2.elb.amazonaws.com/

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  namespace: game-2048
  name: ingress-2048
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c # replace with 3 public subnet ids

spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: service-2048
              port:
                number: 80



#####################################################
# Ingress (Application Load Balancer) Rules
#####################################################
#
# INGRESS
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: color-app-ingress
  namespace: color-app
  labels:
    app: color-app
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
    alb.ingress.kubernetes.io/subnets: subnet-05a3ee7985015400a, subnet-03e978e7176badd78, subnet-0e54fd8bbc66a8e3c # replace with 3 public subnet ids
spec:
  rules:
    - http:
        paths:
          - path: /yellow
            pathType: Prefix
            backend:
              service:
                name: yellow-service
                port:
                  number: 80
          - path: /green
            pathType: Prefix
            backend:
              service:
                name: green-service
                port:
                  number: 80



 7. Delete the application
    unix> kubectl delete -f 2048_full.yaml





Part 3:  Configure the Ingress to use the AWS Load Balancer Controller
----------------------------------------------------------------------
unix> vi  app-load-balancer.yaml

#####################################################
# Ingress (Application Load Balancer) Rules
#####################################################
#
# INGRESS
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: color-app-ingress
  namespace: color-app
  labels:
    app: color-app
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'
    alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'
spec:
  rules:
    - http:
        paths:
          - path: /yellow
            pathType: Prefix
            backend:
              service:
                name: yellow-service
                port:
                  number: 80
          - path: /green
            pathType: Prefix
            backend:
              service:
                name: green-service
                port:
                  number: 80
                                                                                                                                                                                                 132,19        Bot


11. Apply the script
    unix> kubectl apply -f app-load-balancer.yaml









Experimental Stuff
------------------
 1. Create an Application Load Balancer
    a. Go to EC2 -> Load Balancer   (as dev.user)
    b. Press "Create" under "Application Load Balancer"
    c. In Step 1: Configure Load Balancer

       Basic Configuration:
         Name:             app-load-balancer
         Schema:           internet-facing
         IP Address type:  ipv4

       In Listener:
          Load Balancer Protocol:   HTTP
          Load Balancer Port is     80

       Availability Zones:
          VPC:                      project-vpc
          Availability Zones:       Check all 3 zones
                                    Use the dropdowns to select the Public subnet



       Add-onServices
          Go with Defaults


       Press "Next: Configure Security Settings"

       Press "Next: Configure Security Groups"

     d. In Step 3: Configure Security Groups
        select the default VPC security group
        Press "Next: Configure Routing"

     e. In Step 4: Configure Routing

        In Target Group:
               Name:         app-target-group
               Target Type:  instance
               Protocol:     HTTP
               Port:         80

        In health Checks
               Protocol:     HTTP
               Path:         /green

        In Advanced health check settings:
        -- Use the defaults

        Press "Next" Register Targets

      f. In Step 5: Register Targets
         In Instances, select your eks instance, press "Add to registered"
            -- It should appear under "Registered target"

         -- Press "Next: Review"

       g. In Step 6: Review
          Press "Create"


  2. Configure the app-load-balancer
     a. In EC2 -> Load Balancers, chceck app-load-balancer
     b. Click on the "Listeners" tab
     c. Next to HTTP : 80, press "View/edit rules"

     d. Press the "Insert Rule"
     e. In the new rule
        Press + Add condition

        Path is /green  *OR*  /green*

     c. In the "THEN" column press "+Add action"
        Select Forward to....


     In the

















 1. Install the AWS Load Balancer Controller add-on  (formerly known as the AWS ALB Ingress Controller)
    https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html
	Setup the Amazon Certificate Manager (ACM) with our cert for the public facing url -- e.g., app.traderres.com
	   a) Go to aws.com -> ACM
	   b) Press "Import a certificate"
		  In Certificate Body:         Copy the contents of /home/adam/certbot/archive/app.traderres.com/fullchain.part1.pem  (has the actual server cert)
		  In Certificate private key:  Copy the contents of /home/adam/certbot/archive/app.traderres.com/privkey1.pem
		  In Certificate chain:        Copy the contents of /home/adam/certbot/archive/app.traderres.com/fullchain1.pem   (has the chain of all 3)
		  Press "Next"

	   c) In the "Add Tags" page, press "Next"
	   d) In Review and import, press "Import"
	   e) Refresh the Certificates list page
	   f) Click on your new certificate
	   g) Copy the ARN identifier -- e.g., arn:aws-us-gov:acm:us-gov-west-1:527362555097:certificate/b8a49d7f-e65d-4ff0-b533-7a22ab0f6830
	   h) Add this ARN identifier to this line in the app-load-balancer.yaml
				 alb.ingress.kubernetes.io/certificate-arn: arn:aws-us-gov:acm:us-gov-west-1:527362555097:certificate/b8a49d7f-e65d-4ff0-b533-7a22ab0f6830

 2. Download an IAM policy for the AWS Load Balancer Controller that allows it to make calls to AWS APIs on your behalf.
    For AWS GovCloud (US-East) or AWS GovCloud (US-West) AWS Regions
    unix> curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy_us-gov.json

    For All other AWS Regions
    unix> curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.7/docs/install/iam_policy.json

 3. Create an IAM policy using the policy downloaded in the previous step
    unix> aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy_us-gov.json

 4. Create an IAM role
    unix> export MY_ARN=arn:aws-us-gov:iam::527362555097
    unix> export MY_CLUSTER=nccs
    unix> eksctl create iamserviceaccount \
                   --cluster=${MY_CLUSTER} \
                   --namespace=kube-system \
                   --name=aws-load-balancer-controller \
                   --role-name AmazonEKSLoadBalancerControllerRole \
                   --attach-policy-arn=${MY_ARN}:policy/AWSLoadBalancerControllerIAMPolicy \
                   --approve

 5. Install the AWS Load Balancer Controller cert-manager
    unix> kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml

 6. Install the AWS Load Balancer Controller itself
    unix> curl -Lo v2_4_7_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.7/v2_4_7_full.yaml
    unix> sed -i.bak -e '561,569d' ./v2_4_7_full.yaml
    unix> sed -i.bak -e "s|your-cluster-name|${MY_CLUSTER}|" ./v2_4_7_full.yaml
    unix> kubectl apply -f v2_4_7_full.yaml

 7. Download the IngressClass and IngressClassParams manifest to your cluster
    unix> curl -Lo v2_4_7_ingclass.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.7/v2_4_7_ingclass.yaml
    unix> kubectl apply -f v2_4_7_ingclass.yaml


 8. Delete the existing service

 9. Create the yaml file for the ingress and services
    unix> vi app-load-balancer.yaml


#############################################################################
# Filename: app-load-balancer.yaml
#
# Purpose:
#   Setup an Application Load Balancer such
#              https://...front-door.../nccs       --> nccs service
#              Https://...front-door.../nccs-admin --> nccs-admin service
#
#
# Usage:
#   unix> kubectl apply -f app-load-balancer.yaml  # To create the application load balancer
#
#   unix> kubectl describe ingress nccs-ingress    # To see the details
#############################################################################




#####################################################
# nccs-service
#####################################################
apiVersion: v1
kind: Service
metadata:
  name: nccs-service
  labels:
    app: nccs-backend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: /nccs/api/version
spec:
  type: NodePort
  selector:
    app: nccs-backend
  sessionAffinity: ClientIP
  ports:
    - port: 443
      targetPort: 8443


---
#####################################################
# nccs-admin-service
#####################################################
apiVersion: v1
kind: Service
metadata:
  name: nccs-admin-service
  labels:
    app: nccs-admin
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: /nccs-admin/api/version
spec:
  type: NodePort
  selector:
    app: nccs-admin
  sessionAffinity: ClientIP
  ports:
    - port: 443
      targetPort: 8443



---
#####################################################
# Ingress (Application Load Balancer) Rules
#####################################################
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nccs-app-ingress
  labels:
    app: nccs-app
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/subnets: subnet-0d6b3475610db5c29, subnet-0f4fb1cefe2a43c4c, subnet-0a67a57f98ec036a4
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS": 443}]'
    alb.ingress.kubernetes.io/ip-address-type: ipv4
    alb.ingress.kubernetes.io/certificate-arn: arn:aws-us-gov:acm:us-gov-west-1:527362555097:certificate/b8a49d7f-e65d-4ff0-b533-7a22ab0f6830
    alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'
    alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'
    alb.ingress.kubernetes.io/success-codes: '200'
    alb.ingress.kubernetes.io/healthy-threshold-count: '2'

spec:
  rules:
    - host: nccs.rbr-tech.com
    - http:
        paths:
          - path: /nccs-admin
            pathType: Prefix
            backend:
              service:
                name: nccs-admin-service
                port:
                  number: 443
          - path: /nccs
            pathType: Prefix
            backend:
              service:
                name: nccs-service
                port:
                  number: 443

11. Apply the script
       unix> kubectl apply -f app-load-balancer.yaml

12. Verify that the load balancer exists
       unix> kubectl get ingress nccs-app-ingress

       NAME               CLASS    HOSTS               ADDRESS                                                                     PORTS   AGE
       nccs-app-ingress   <none>   nccs.rbr-tech.com   k8s-default-nccsappi-63918f0120-268589748.us-gov-west-1.elb.amazonaws.com   80      3m40s


13. Verify that the controller is installed
           unix> kubectl get deployment -n kube-system aws-load-balancer-controller

           NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
           aws-load-balancer-controller   1/1     1            1           55s


           unix> kubectl describe ingress nccs-app-ingress

				Name:             nccs-app-ingress
				Labels:           app=nccs-app
				Namespace:        default
				Address:
				Ingress Class:    <none>
				Default backend:  <default>
				Rules:
				  Host        Path  Backends
				  ----        ----  --------
				  *
							  /nccs-admin   nccs-admin-service:443 (172.31.43.208:8443)
							  /nccs         nccs-service:443 (172.31.35.149:8443)
				Annotations:  alb.ingress.kubernetes.io/certificate-arn: arn:aws-us-gov:acm:us-gov-west-1:527362555097:certificate/b8a49d7f-e65d-4ff0-b533-7a22ab0f6830
							  alb.ingress.kubernetes.io/ip-address-type: ipv4
							  alb.ingress.kubernetes.io/listen-ports: [{"HTTPS": 443}]
							  alb.ingress.kubernetes.io/scheme: internet-facing
							  alb.ingress.kubernetes.io/subnets: subnet-0d6b3475610db5c29, subnet-0f4fb1cefe2a43c4c, subnet-0a67a57f98ec036a4
							  alb.ingress.kubernetes.io/target-type: ip
							  kubernetes.io/ingress.class: alb
				Events:       <none>

14. Get the hostname of the load balancer and then change that in your godaddy.com

          unix> kubectl get ingress nccs-app-ingress

          NAME               CLASS    HOSTS               ADDRESS                                                                     PORTS   AGE
          nccs-app-ingress   <none>   nccs.rbr-tech.com   k8s-default-nccsappi-63918f0120-268589748.us-gov-west-1.elb.amazonaws.com   80      3m40s

         Go into godaddy.com and change app.traderres.com CNAME to use the same address 8s-default-nccsappi-63918f0120-268589748.us-gov-west-1.elb.amazonaws.com


15. Try to use the load balancer
          a. Go to https://nccs.rbr-tech.com/nccs

16. Show logging on ingress
         unix>  kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller

17. Describe the deployment
         unix>  kubectl describe deployment aws-load-balancer-controller -n kube-system

18. ssh into one of the running pods
         unix> kubectl exec nccs-deployment-5ddcd7fd5d-nnlmh /bin/bash -it
