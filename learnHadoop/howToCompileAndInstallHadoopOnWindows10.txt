How to Compile and Install a Native Hadoop 2.9.2 on Windows 10
--------------------------------------------------------------
This articles desribes how to build a binary native distribution of hadoop for Windows 7 or Windows 10

Apache Hadoop 2.2.0 and later support running Hadoop on Microsoft Windows.
But, the "bin" distribution of Apache Hadoop 2.2.0 release does *NOT* contain some windows native components (like winutils.exe, hadoop.dll etc).
-- As a result, if we need to access Hadoop libraries from compiled Java code, we will encounter ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path.


Assumptions:
 A) You are running Windows 7 or Windows 10
 B) You have Administrative access to your box


Procedures
----------
 1. Create a directory called "C:\tools"


 2. Download and install 7zip
    NOTE:  We use 7zip to pull the contents out of ISO files
    a. go to http://www.7-zip.org/
    b. Click on the 64-bit x64 installer
       *OR*
       Go to https://www.7-zip.org/7z1900-x64.exe
    c. Save 7z1900-x64.exe to your "Downloads" directory
    d. Go to your "Downloads" directory and run 7z1900-x64.exe
    e. Install 7zip


 3. Download and install Microsoft Visual Studio 2010 Professional Edition    (en_visual_studio_2010_professional.iso)
    NOTE:  Hadoop needs this older compiler to compile it for Windows
    a. Download en_visual_studio_2010_professional.iso
       https://drive.google.com/open?id=1G9kvgbKAVqbRV8T03sp39YZa_2xdhGJN
    b. Save it to your Downloads Directory
    c. Right-click on en_visual_studio_2010_professional.iso -> 7zip -> Extract to en_visual_studio_2010_professional.iso\
    d. Run .\en_visual_studio_2010_professional.iso\setup.exe
       1) Click "Install Microsoft Visual Studio 2010"
       2) Uncheck "Yes, send information about my setup experience to Microsoft Corporation"
          Press Next
       3) In the License Terms screen, press "I have read and accept the license terms"
          Press Next
       4) In the Installation Options screen,
          Select "Full"
          Press Install
          -- Wait up to 30 minutes for the installation to finish

       5) In the final page, Press "Finish"
       6) Press Exit


 4. Download and install Microsoft Visual Studio 2010 Service Pack 1   (en_visual_studio_2010_sp1_x86_dvd_651704.iso)
    a. Download en_visual_studio_2010_sp1_x86_dvd_651704.iso
       https://drive.google.com/open?id=1jPZtlUTXskKKzxAINM_r1Wf9Yc2RfV1i
    b. Save it to your "Downloads" directory
    c. Go to your "Downloads" directory
    d. Right-click on en_visual_studio_2010_sp1_x86_dvd_651704.iso -> 7zip -> Extract to en_visual_studio_2010_sp1_x86_dvd_651704\
    e. Run .\en_visual_studio_2010_sp1_x86_dvd_651704\Setup.exe
       1) In the first screen, Press Next
       2) In the next screen, Press "I have read and accept the license terms", Press Install
          -- Wait up to 30 minutes for the installation to finish

       3) In the Installation is complete screen, press Finish
       4) If you are prompted to restart your computer, then restart now.


 5. Set the environment variable called "Platform" to x64
    NOTE:  The Microsoft SDK uses this to determine whether to compile 32-bit or 64-bit
           We are setting it to tell the compiler to compile for 64-bit

    a) Open the Environment Variables in Windows by pressing <Start><Run>environment
    b) Under "User variables for..." [on the top of this popup],
       click "New..."
             Variable name:  Platform
             Variable value: x64

       NOTES:
           Variable name "Platform" is case sensitive.
           The Platform environment value must be either x64 or Win32


 6. Download Protocol Buffers 2.5.0  (Windows binaries)
    WARNING:  The Hadoop code looks for version 2.5.0 so do *NOT* download a later version
    a. Go to https://repo1.maven.org/maven2/com/google/protobuf/protoc/2.5.0/protoc-2.5.0-windows-x86_64.exe
    b. Rename protoc-2.5.0-windows-86_64.exe to be protoc.exe
    c. Create this directory path:
         c:\tools\protobuf\bin

    d. Save protoc.exe to c:\tools\protobuf\bin\protoc.exe
    e. Add c:\tools\protobuf\bin to your PATH
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."  [if PATH already exists, then double-click on PATH and add a semicolon and append this to the end of the PATH]
                Variable name:  PATH
                Variable value: c:\tools\protobuf\bin

    f. Verify protoc is in your path and is version 2.5.0
       1) Open a DOS window and verify protoc is found in the path
       2) Enter this in the DOS window:
           CMD> protoc --version
           libprotoc 2.5.0


 7. Download and install OpenJDK 1.8 64-bit  (You must get the x64 MSI Installer)
    a. Go to https://developers.redhat.com/products/openjdk/download
    b. Scroll down in the page
       -- Select to download the OpenJDK 8 Windows x64 MSI installer
       -- NOTE:  The file you want to download should end with .MSI

    c. You will be prompted to login with your RedHat account
       1) Click "Create one now"
       2) Fill-in the form to create an account
       3) Go to your email account
          Click on the email no-reply@redhat.com
          Click on the link to verify your email
       4) Next, you should see a link to download
       5) Save the MSI file to your Downloads directory
       6) Go to your Downloads directory and double-click on the installer
          1) In the Welcome screen, press Next
          2) In the End-User License Agreement screen, click "I accept" and press Next
          3) In the Custom Setup screen, use the defaults and press Next
          4) In the Ready to install screen, press Install

    d. Add JAVA_HOME as an environment variable
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."
                 Variable name:  JAVA_HOME
                 Variable value: C:\progra~1\Redhat\java-1.8.0-openjdk-1.8.0.222-4

                 NOTE:  Verify that this is the *correct* directory path

       For example:
           This is good:   C:\progra~1\Redhat\java-1.8.0-openjdk-1.8.0.222-4
           This is bad:    C:\Program Files\Redhat\java-1.8.0-openjdk-1.8.0.222-4

    
	
	NOTE:  ALSO add the JAVA_HOME as a **SYSTEM VARIABLE**
	
	
 8. Download and install Maven 3.3.1
    a. Download Maven apache-maven-3.3.1-bin.zip
       1) Go to https://maven.apache.org/download.cgi
       2) Look for apache-maven-3.3.1-bin.zip and click on it
          *OR*
          Go to http://archive.apache.org/dist/maven/maven-3/3.3.1/binaries/apache-maven-3.3.1-bin.zip

       3) Save apache-maven-3.3.1-bin.zip to your Downloads
       4) Unzip apache-maven-3.3.1-bin.zip to your c:\tools
          a) Go to your Downloads directory
          b) Right-click on apache-maven-3.3.1-bin.zip -> Extract All...
             In the Extract All
               C:\tools
               Press "Extract"
               -- Now, you should have the directory c:\tools\apache-maven-3.3.1

    b. Create an environment variable called M2_HOME=c:\tools\apache-maven-3.3.1
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."
                  Variable name:  M2_HOME
                  Variable value: c:\tools\apache-maven-3.3.1

    c. Add the c:\tools\apache-maven-3.3.1 to your PATH
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."  [if PATH already exists, then double-click on PATH and add a semicolon and append this to the end of the PATH]
                Variable name:  PATH
                Variable value: c:\tools\apache-maven-3.3.1\bin

    d. Verify that maven works
       1) Press <Start><Run>CMD
       2) In the dos window, type-in this:
          DOS> mvn --version
          -- You should see that it says Apache Maven 3.3.1


 9. Download & Install Cygwin    (You need bash to compile the Hadoop binaries and cygwin provides it for Windows)
    a. Go to https://cygwin.com/setup-x86_64.exe
    b. Save "setup-x86_64.exe" to your Downloads directory
    c. Go to your Downloads directory and double-click on "setup-x86_64.exe"
       1) In the first popup, press Next
       2) In the "Choose a Download Source", select "Install from Internet" and press Next
       3) In the "Select Root Install Directory,
             Root Directory:  c:\cygwin
             Install for:     All Users
             Press Next
       4) In the Setup Local Package Directory, go with defaults and press Next
       5) In the "Select your Internet Connection", use System Proxy Settings and press Next
       6) In the "Choose a Download Site", choose anyone and press Next
       7) In the "Select Packages", press "Next"
       8) In the "Review and confirm changes", press "Next"
             -- Wait about 10 minutes
       9) In the "Create Icons", press "Finish"


    d. Make sure the c:\cygwin\bin directory is in your PATH
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          -- Single Click on "Path" and press "Edit
          -- Press "New"
                  Variable name:  PATH
                  Variable value: c:\cygwin\bin


10. Download and install CMake 3.15.2
    a. Go to http://www.cmake.org/download/
    b. Scroll down to Binary distributions
       -- Look for the Windows win64-x64 install
       1) Click on cmake-3.15.3-win64-x64.msi
       2) Save cmake-3.15.3-win64-x64.msi to your "Downloads" directory
       3) Go to your "Downloads" directory and double-click on "cmake-3.15.3-win64-x64.msi"
          a) On the Welcome screen, press Next
          b) On the License agreement screen, click "I accept" and press Next
          c) On the Install options, click "Add CMake to the system path for current user"
             press Next
          d) On the "Choose Install Location",
             Set the destination folder to c:\tools\cmake
             press Next
          e) On the "Ready to install cmake" screen, press Install
          f) On the "Completed Setup" screen, Press Finish


11. Download and install the zlib compiled DLL and include files
    a. Go to http://www.zlib.net/
       Scroll down to  zlib compiled DLL, version....
       *OR*
       Go to http://zlib.net/zlib128-dll.zip

    b. Save zlib128-dll.zip to your "Downloads" directory
    c. Go to your "Downloads" Directory
       1) Right-click zlib128-dll.zip -> Extract All...
       2) Extract To:  c:\tools\zlib-1.2.8\
          Press Extract

    d. Add C:\tools\zlib-1.2.8 to your PATH
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          -- Single Click on "Path" and press "Edit
          -- Press "New"
                 Variable name:  PATH
                 Variable value: C:\tools\zlib-1.2.8

    e. Create an environment variable called ZLIB_HOME=C:\tools\zlib-1.2.8\include
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."
                 Variable name:  ZLIB_HOME
                 Variable value: C:\tools\zlib-1.2.8\include


12. Download and install OpenSSL 1.0.1p with development files for Windows
    a. Go to https://wiki.openssl.org/index.php/Binaries (to find the openssl Windows binaries)
    b. Go to https://slproweb.com/products/Win32OpenSSL.html
    c. Select "Win64 OpenSSL v1.1.1c" EXE  (not the version that says Light)
       *OR*
       Go to https://slproweb.com/download/Win64OpenSSL-1_1_1c.exe

    d. Save Win64OpenSSL-1_1_1c.exe to your Downloads
    e. Go to your Downloads and double-click on Win64OpenSSL-1_1_1c.exe
       1) If you get the popup that says
          "The Win64 OpenSSL Installation Project setup has detected that
           Microsoft Visual C++ 2017 Redistributable (64-bit) is missing"
           a) Click Yes (to download this now)
           b) Save vc_redist.x64.exe to your Dowloads
           c) Go to your Downloads and run vc_redist.x64.exe
              1) In the welcome popup, click "I agree to the license terms and conditions"
                 and press "Install"
              2) Press "Close"

       1) In the License Agreement screen (for OpenSSL 1.1.1c), select "I accept the agreement" and press "Next"
       2) In the "Select destination location" screen
             Set the destination to c:\tools\openssl-1.1.1c
             Press Next
       3) In the "Select Start Menu Folder" screen, use the defaults and press Next
       4) In the "Select Additional Tasks", choose "The Openssl binaries /bin directory" and press Next
       5) In the "Ready to Install" screen, press Install
       6) In the last popup, uncheck the options and press Finish


13. Add the msbuild from your Visual Studio 2019 to your path (so that maven can find msbuild)
    a. Open the Environment Variables in Windows by pressing <Start><Run>environment
    b. Under "User variables for..." [on the top of this popup],
           -- Single Click on "Path" and press "Edit
           -- Press "New"
                  Variable name:  PATH
                  Variable value: C:\Windows\Microsoft.NET\Framework64\v4.0.30319


14. Download hadoop 2.9.2 source files to your c:\tools
    a. Open a browser and go to https://hadoop.apache.org/releases.html
    b. Click on the "source download" link next to a version 2.9.2
       *OR*
       Go to http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.2/hadoop-2.9.2-src.tar.gz

    c. Click on one of the mirrors
    d. Save hadoop-2.9.2-src.tar.gz to your "Downloads" directory
    e. Extract to your c:\tools\hadoop-2.9.2-src\ directory
       -- Go to your Downloads directory
       -- Right-click on hadoop-2.9.2-src.tar.gz  -> 7zip -> Extract Here....
       -- Right-click on hadoop-2.9.2-src.tar     -> 7zip -> Extract files....
          Extract To:  c:\tools   (and uncheck the hadoop-2.9.2-src\)
          Press OK
          -- Now, you should have hadoop source located here:
              c:\tools\hadoop-2.9.2-src\hadoop-assemblies
              c:\tools\hadoop-2.9.2-src\hadoop-build-tools
              c:\tools\hadoop-2.9.2-src\hadoop-client
              c:\tools\hadoop-2.9.2-src\hadoop-cloud-storage-project
              ...

14b. Apply a patch so that Hadoop's Yarn does not check file permissions
     a. Edit this file:  ResourceLocalizationService.java
	
	   C:\tools\hadoop-2.9.2-src\hadoop-yarn-project\hadoop-yarn\hadoop-yarn-server\hadoop-yarn-server-nodemanager\src\main\java\org\apache\hadoop\yarn\server\nodemanager\containermanager\localizer\ResourceLocalizationService.java
	   

     b. Change this method by commenting out the second if statement

			private boolean checkLocalDir(String localDir) {

				Map<Path, FsPermission> pathPermissionMap = getLocalDirsPathPermissionsMap(localDir);

				for (Map.Entry<Path, FsPermission> entry : pathPermissionMap.entrySet()) {
				  FileStatus status;
				  try {
					status = lfs.getFileStatus(entry.getKey());
				  } catch (Exception e) {
					String msg =
						"Could not carry out resource dir checks for " + localDir
							+ ", which was marked as good";
					LOG.warn(msg, e);
					throw new YarnRuntimeException(msg, e);
				  }

			//      if (!status.getPermission().equals(entry.getValue())) {
			//        String msg =
			//            "Permissions incorrectly set for dir " + entry.getKey()
			//                + ", should be " + entry.getValue() + ", actual value = "
			//                + status.getPermission();
			//        LOG.warn(msg);
			//        throw new YarnRuntimeException(msg);
			//      }
				}
				return true;
			  }



15. Compile Hadoop binaries for Windows
    a. Open a Windows Command Prompt *AS AN ADMINISTRATOR*
       Type-in CMD
       Right-click on Command Prompt -> Run as Administrator

    b. In the Administrator console, type-in these commands
       DOS> "c:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\vcvarsall.bat" x64
       Setting environment for using Microsoft Visual Studio 2010 x64 tools

	   -- Add these to your path
	   DOS> set PATH=c:\cygwin\bin;%PATH%
	   DOS> set PATH=c:\tools\protobuf\bin;%PATH%
	   DOS> set PATH=c:\tools\cmake\bin;%PATH%
	   
	   -- Compile all of the Hadoop libraries
	   DOS> cd /d c:\tools\hadoop-2.9.2-src
       DOS> mvn clean package -Pdist,native-win -DskipTests -Dtar
       -- Wait up to 30 minutes for it to compile

       -- If this build fails, make sure your PATH was setup correctly in the previous steps
       -- If this build succeeds, the hadoop binaries should be installed here:
             C:\tools\hadoop-2.9.2-src\hadoop-dist\target\hadoop-2.9.2.tar.gz


16. Install the hadoop binaries to c:\tools\hadoop-2.9.2
    a. Browse to C:\tools\hadoop-2.9.2-src\hadoop-dist\target
    b. Right-click on hadoop-2.9.2.tar.gz -> 7-zip -> Extract Here
    c. Right-click on hadoop-2.9.2.tar    -> 7-zip -> Extract files...
          In the Extract to:  c:\tools
          Press OK

          Now, you should have hadoop files installed to here:
             C:\tools\hadoop-2.9.2

    b. Set environment variable  HADOOP_HOME = c:\tools\hadoop-2.9.2
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          Click "New..."
                 Variable name:  HADOOP_HOME
                 Variable value: c:\tools\hadoop-2.9.2

    c. Add c:\tools\hadoop-2.9.2\bin to your PATH
       1) Open the Environment Variables in Windows by pressing <Start><Run>environment
       2) Under "User variables for..." [on the top of this popup],
          -- Single Click on "Path" and press "Edit
          -- Press "New"
                 Variable name:  PATH
                 Variable value: c:\tools\hadoop-2.9.2\bin
          -- Press OK a few times to close it up

    d. Verify that hadoop is found
       1) Open a DOS window by pressing <Start><Run>CMD
       2) In the DOS window, type-in this:
          DOS> hadoop version

          You should see the following:
          Hadoop 2.9.2
          Subversion Unknown -r Unknown
          Compiled by Adam on 2015-07-26T03:20Z
          Compiled with protoc 2.5.0
          From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
          This command was run using /C:/tools/hadoop-2.9.2/share/hadoop/common/hadoop-common-2.9.2.jar


     WARNING!!!!  IF YOU GET ERRORS, make sure your JAVA_HOME has no spaces in it
	                       JAVA_HOME=C:\progra~1\RedHat\Java-1.8.0-openjdk-1.8.0.282-1
						   
	 

17. Configure Hadoop to run in pseudo-distributed mode
    a. Edit the C:\tools\hadoop-2.9.2\etc\hadoop\core-site.xml
        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

            <configuration>
                <property>
                    <name>fs.defaultFS</name>
                    <value>hdfs://localhost:9000</value>
                </property>
            </configuration>


    b. Edit the C:\tools\hadoop-2.9.2\etc\hadoop\hdfs-site.xml
       Note:  Create namenode and datanode directory under c:/hadoop/data/dfs/.
       Note:  Set the datanode to listen on port 50001

        <?xml version="1.0" encoding="UTF-8"?>
        <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

            <configuration>
                <property>
                    <name>dfs.replication</name>
                    <value>1</value>
                </property>
                <property>
                    <name>dfs.namenode.name.dir</name>
                    <value>file:/hadoop/data/dfs/namenode</value>
                </property>
                <property>
                    <name>dfs.datanode.data.dir</name>
                    <value>file:/hadoop/data/dfs/datanode</value>
                </property>

                <property>
                    <name>dfs.datanode.address</name>
                    <value>localhost:50001</value>
                </property>
				
				<property>
                    <name>dfs.namenode.http-address</name>
                    <value>0.0.0.0:50070</value>
                </property>
	
				<property>
                    <name>dfs.datanode.http.address</name>
                    <value>0.0.0.0:50075</value>
                </property>	
            </configuration>


    c. Edit the C:\tools\hadoop-2.9.2\etc\hadoop\yarn-site.xml

        <?xml version="1.0"?>

         <configuration>
            <property>
               <name>yarn.nodemanager.aux-services</name>
               <value>mapreduce_shuffle</value>
            </property>
            <property>
               <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
            <property>
               <name>yarn.application.classpath</name>
               <value>
                    %HADOOP_HOME%\etc\hadoop,
                    %HADOOP_HOME%\share\hadoop\common\*,
                    %HADOOP_HOME%\share\hadoop\common\lib\*,
                    %HADOOP_HOME%\share\hadoop\mapreduce\*,
                    %HADOOP_HOME%\share\hadoop\mapreduce\lib\*,
                    %HADOOP_HOME%\share\hadoop\hdfs\*,
                    %HADOOP_HOME%\share\hadoop\hdfs\lib\*,
                    %HADOOP_HOME%\share\hadoop\yarn\*,
                    %HADOOP_HOME%\share\hadoop\yarn\lib\*
               </value>
            </property>
        </configuration>


    d. Edit the C:\tools\hadoop-2.9.2\etc\hadoop\mapred-site.xml
         1) Copy mapred-site.xml.template to mapred-site.xml

         2) Copy this to the mapred-site.xml file

             <?xml version="1.0"?>
             <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

             <configuration>
                <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
                </property>
             </configuration>


18. Format the Name Node
    Open a DOS window in Administrator Mode
    DOS> cd %HADOOP_HOME%\bin
    DOS> hdfs namenode -format

        You should see this:

        STARTUP_MSG:   build = Unknown -r Unknown; compiled by 'adam' on 2015-07-26T16:37Z
        STARTUP_MSG:   java = 1.7.0_60
        ************************************************************/
        15/07/26 12:55:47 INFO namenode.NameNode: createNameNode [-format]
        Formatting using clusterid: CID-e29bd4ce-2e2e-415e-90a6-b2f627f61b0d
        15/07/26 12:55:48 INFO namenode.FSNamesystem: No KeyProvider found.
        15/07/26 12:55:48 INFO namenode.FSNamesystem: fsLock is fair:true
        15/07/26 12:55:48 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
        15/07/26 12:55:48 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:0.000
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: The block deletion will start around 2015 Jul 26 12:55:48
        15/07/26 12:55:48 INFO util.GSet: Computing capacity for map BlocksMap
        15/07/26 12:55:48 INFO util.GSet: VM type       = 64-bit
        15/07/26 12:55:48 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
        15/07/26 12:55:48 INFO util.GSet: capacity      = 2^21 = 2097152 entries
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: defaultReplication         = 1
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: maxReplication             = 512
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: minReplication             = 1
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
        15/07/26 12:55:48 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
        15/07/26 12:55:48 INFO namenode.FSNamesystem: fsOwner             = adam (auth:SIMPLE)
        15/07/26 12:55:48 INFO namenode.FSNamesystem: supergroup          = supergroup
        15/07/26 12:55:48 INFO namenode.FSNamesystem: isPermissionEnabled = true
        15/07/26 12:55:48 INFO namenode.FSNamesystem: HA Enabled: false
        15/07/26 12:55:48 INFO namenode.FSNamesystem: Append Enabled: true
        15/07/26 12:55:48 INFO util.GSet: Computing capacity for map INodeMap
        15/07/26 12:55:48 INFO util.GSet: VM type       = 64-bit
        15/07/26 12:55:48 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
        15/07/26 12:55:48 INFO util.GSet: capacity      = 2^20 = 1048576 entries
        15/07/26 12:55:48 INFO namenode.FSDirectory: ACLs enabled? false
        15/07/26 12:55:48 INFO namenode.FSDirectory: XAttrs enabled? true
        15/07/26 12:55:48 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
        15/07/26 12:55:48 INFO namenode.NameNode: Caching file names occuring more than 10 times
        15/07/26 12:55:48 INFO util.GSet: Computing capacity for map cachedBlocks
        15/07/26 12:55:48 INFO util.GSet: VM type       = 64-bit
        15/07/26 12:55:48 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
        15/07/26 12:55:48 INFO util.GSet: capacity      = 2^18 = 262144 entries
        15/07/26 12:55:48 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
        15/07/26 12:55:48 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
        15/07/26 12:55:48 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
        15/07/26 12:55:48 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
        15/07/26 12:55:48 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
        15/07/26 12:55:48 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
        15/07/26 12:55:48 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
        15/07/26 12:55:48 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
        15/07/26 12:55:48 INFO util.GSet: Computing capacity for map NameNodeRetryCache
        15/07/26 12:55:48 INFO util.GSet: VM type       = 64-bit
        15/07/26 12:55:48 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
        15/07/26 12:55:48 INFO util.GSet: capacity      = 2^15 = 32768 entries
        15/07/26 12:55:48 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1500842503-192.168.1.2-1437929748421
        15/07/26 12:55:48 INFO common.Storage: Storage directory \hadoop\data\dfs\namenode has been successfully formatted.
        15/07/26 12:55:48 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
        15/07/26 12:55:48 INFO util.ExitUtil: Exiting with status 0
        15/07/26 12:55:48 INFO namenode.NameNode: SHUTDOWN_MSG:
        /************************************************************
        SHUTDOWN_MSG: Shutting down NameNode at anonymous-PC/192.168.1.2
        ************************************************************/

    A few notes about the output shown above:
      A) "HA Enabled: false"
          means that high availablity is not enabled
      B) "Storage directory \hadoop\data\dfs\namenode has been successfully formatted."
          means that A local directory is being formatted for the name node:


19. Start HDFS (Start the Name Node and Data Node)
    DOS> cd %HADOOP_HOME%\sbin
    DOS> start-dfs
         You should see 2 DOS windows open up
         1) Apache Hadoop Distribution - hadoop namenode
         2) Apache Hadoop Distribution - hadoop datanode


    NOTE:  If you get address already in use errors, you may have to change the ports found in this file:
	            C:\tools\hadoop-2.9.2\etc\hadoop\hdfs-site.xml
		   And, then try the start-dfs command again.
		   

20. Start MapReduce aka YARN (Resource Manager and Node Manager)
    DOS> cd %HADOOP_HOME%\sbin
    DOS> start-yarn
         You should see 2 DOS windows open up
         1) Apache Hadoop Distribution - yarn nodemanager
         2) Apache Hadoop Distribution - yarn resourcemanager


21. Verify it is running
    a. Go to the NodeManager's url:
       http://localhost:8042/  *OR*  http://localhost:8033/
       -- You should see the Hadoop Node Manager page

    b. Go to the Namenode's website
       http://localhost:50070
       -- You should see the Hadoop Overview page


22. Create a directory with your Hadoop File System
    a. List what is in your HDFS top directory
       DOS> hadoop fs -ls /
            NOTE:  It should display nothing

    b. Create a directory called /tmp in HDFS
       DOS> hadoop fs -mkdir /tmp
       DOS> hadoop fs -chmod ugo+rwx /tmp
       DOS> hadoop fs -ls /
            Found 1 items
            drwxrwxrwt   - adam supergroup          0 2015-07-26 18:52 /tmp


    c. Insert c:\temp\stuff.txt into your HDFS /tmp/stuff.txt
       1) Create a file called c:\temp\stuff.txt

       2) Use the hadoop command-line to insert it into your HDFS
          DOS> hadoop fs -put c:\temp\stuff.txt /tmp/stuff.txt

          DOS> hadoop fs -ls /tmp
               Found 1 items
               -rw-r--r--   1 adam supergroup          6 2015-07-26 18:56 /tmp/stuff.txt

       3) Use the hadoop command-line to get an HDFS file and copy it to your local file system
          DOS> hadoop fs -get /tmp/stuff.txt c:\temp\stuff2.txt
          DOS> type c:\temp\stuff2.txt
          -- You should see the contents of your file

       4) Display the content's of a file located in HDFS
          DOS> hadoop fs -cat /tmp/stuff.txt
          -- You should see the contents of your file


23. Stop everything
    DOS> cd %HADOOP_HOME%\sbin
    DOS> stop-all


NOTE:  You can use start-all (to start everything) but it is deprecated.



Setup Standalone Mode for debugging
-----------------------------------
In core-site.xml
    fs.default.name is file:///   (defailt)

In hdfs-site.xml
    dfs.replication is not set

In mapreduce-site.xml
    mapred.job.tracker is local   (default)

