How to Clear Hadoop Disk Space in BDP
-------------------------------------
Problem:  The RDA Deployer is not working because we're low on disk space in Hadoop
Solution: Erase a bunch of files in Hadoop

References
----------
https://www.informit.com/articles/article.aspx?p=2755708&seqNum=4


Procedure
---------
 1. Connect to a data node on the cluster
    a. ssh to the puppet master
    b. Run these commands:
       unix> go datanodes

       unix> sudo -s
       unix> su - yarn    # become the yarn user so we can delete anything in Hadoop


 2. Use the yarn user to delete a bunch of files
    a. Get the total space available to HDFS
       unix> hadoop fs -df -h

    b. Get the total amount of space consumed by HDFS   *BEFORE* deleting files
       unix> hadoop fs -du -s -h /

    c. Determine which directories are using the most amount of space
       unix> hadoop fs -du -h -s /accumulo /bdp-ingest /pg_backups /pg_log_backups /rda /storage /tmp /user /var

    d. Delete some BDP files
       unix> hadoop fs -rm -r -skipTrash /rda/backups/*
       unix> hadoop fs -rm -r -skipTrash /pg_log_backups/*       # delete postgres log backups
       unix> hadoop fs -rm -r -skipTrash /pg_backups/*           # delete postgres backups

    e. Get the total amount of space consumed by HDFS   *AFTER* deleting files
       unix> hadoop fs -du -s -h /

    f. Get the total space available to HDFS
       unix> hadoop fs -df -h



Storage Statistics from the dfsadmin Command
--------------------------------------------
 unix> hdfs dfsadmin -report